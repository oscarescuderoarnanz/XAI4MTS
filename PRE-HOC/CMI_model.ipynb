{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3224698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "import warnings\n",
    "import os\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy.linalg as la\n",
    "from numpy import log\n",
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b1a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reset():\n",
    "    f =  ['AMG', 'CAR', 'CF1', 'CF3', 'CF4',\n",
    "            'Others', 'GLI', 'LIN', 'LIP', 'MAC', 'NTI', 'OXA', 'PAP', 'PEN', 'POL',\n",
    "            'QUI', 'SUL', 'MV hours', 'Acinet$_{pc}$', 'Enterobac$_{pc}$', 'Enteroc$_{pc}$',\n",
    "            'Pseud$_{pc}$', 'Staph$_{pc}$', 'Others$_{pc}$', 'ICU$_{hours}$',\n",
    "            '# pat$_{atb}$', '# pat$_{MDR}$',\n",
    "            'CAR$_{n}$', 'PAP$_{n}$', \n",
    "            'Others$_{n}$', 'QUI$_{n}$',\n",
    "            'OXA$_{n}$', 'PEN$_{n}$', 'CF3$_{n}$', 'GLI$_{n}$',\n",
    "            'CF4$_{n}$', 'SUL$_{n}$', 'NTI$_{n}$', 'LIN$_{n}$',\n",
    "            'AMG$_{n}$', 'MAC$_{n}$', 'CF1$_{n}$', 'POL$_{n}$',\n",
    "            'LIP$_{n}$', '# pat$_{tot}$' ,'Post change',\n",
    "            'Insulin', 'Art nutrition', 'Sedation', 'Relax', 'Hepatic$_{fail}$',\n",
    "            'Renal$_{fail}$', 'Coagulation$_{fail}$', 'Hemodynamic$_{fail}$',\n",
    "            'Respiratory$_{fail}$', 'Multiorganic$_{fail}$',  '# transfusions',\n",
    "            'Vasoactive drug', 'Dosis nems', 'Tracheo$_{hours}$', 'Ulcer$_{hours}$',\n",
    "            'Hemo$_{hours}$', 'C01 PIVC 1',\n",
    "            'C01 PIVC 2', 'C02 CVC - RJ',\n",
    "            'C02 CVC - RS', 'C02 CVC - LS', 'C02 CVC - RF',\n",
    "            'C02 CVC - LJ', 'C02 CVC - LF', '# catheters']\n",
    "\n",
    "\n",
    "    tf =  ['discreta', 'discreta', 'discreta', 'discreta', 'discreta', \n",
    "                        'discreta', 'discreta', 'discreta', 'discreta', 'discreta',\n",
    "                        'discreta', 'discreta', 'discreta', 'discreta', 'discreta',\n",
    "                        'discreta', 'discreta', 'continua', 'discreta',  \n",
    "                        'discreta', 'discreta', 'discreta', 'discreta', 'discreta', 'continua',\n",
    "                        'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'discreta', 'discreta',\n",
    "                        'discreta', 'discreta', 'discreta', 'discreta', 'discreta',\n",
    "                        'discreta', 'discreta', 'discreta',\n",
    "                        'discreta', 'continua', 'discreta', 'continua',\n",
    "                        'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua', 'continua', 'continua',\n",
    "                        'continua', 'continua', 'continua','continua']\n",
    "    \n",
    "    return f, tf\n",
    "\n",
    "### Functions to separate MDR and Non MDR patients ###########################\n",
    "\n",
    "def prepare_amr(split_num, features, norm):\n",
    "    \n",
    "    X_train = np.load(f\"../../DATA/BBCET_cult_ok/s{split_num}/X_train_tensor_{norm}.npy\")\n",
    "    y_train = pd.read_csv(f\"../../DATA/BBCET_cult_ok/s{split_num}/y_train_tensor_{norm}.csv\")\n",
    "    \n",
    "    y_train_aux = y_train[y_train.individualMRGerm != 666].reset_index(drop=True)\n",
    "    y_train_aux = y_train_aux.groupby(by=\"Admissiondboid\").sum().reset_index()\n",
    "    amr = y_train_aux[y_train_aux.individualMRGerm != 0].index\n",
    "    \n",
    "    X_train_amr = X_train[amr]\n",
    "    P, T, F = X_train.shape\n",
    "    y_train_values = y_train[['individualMRGerm']].values.flatten()\n",
    "    y_train_amr = y_train_values.reshape((P, T))\n",
    "    y_train_amr = y_train_amr[amr]\n",
    "    \n",
    "    dfs = []\n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(X_train_amr[:, t, :], columns=[f'{feature}_{t}' for feature in features])\n",
    "        dfs.append(temp_df)\n",
    "    final_df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    dls = []\n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(y_train_amr[:, t], columns=[t])\n",
    "        dls.append(temp_df)\n",
    "    final_dl = pd.concat(dls, axis=1)\n",
    "    \n",
    "    return final_df, final_dl, T, F\n",
    "\n",
    "def prepare_noamr(split_num, features, norm):\n",
    "    \n",
    "    X_train = np.load(f\"../../DATA/BBCET_cult_ok/s{split_num}/X_train_tensor_{norm}.npy\")\n",
    "    y_train = pd.read_csv(f\"../../DATA/BBCET_cult_ok/s{split_num}/y_train_tensor_{norm}.csv\")\n",
    "\n",
    "    y_train_aux = y_train[y_train.individualMRGerm != 666].reset_index(drop=True)\n",
    "    y_train_aux = y_train_aux.groupby(by=\"Admissiondboid\").sum().reset_index()\n",
    "    noamr = y_train_aux[y_train_aux.individualMRGerm == 0].index\n",
    "    \n",
    "    X_train_noamr = X_train[noamr]\n",
    "    P, T, F = X_train.shape\n",
    "    y_train_values = y_train[['individualMRGerm']].values.flatten()\n",
    "    y_train_noamr = y_train_values.reshape((P, T))\n",
    "    y_train_noamr = y_train_noamr[noamr]\n",
    "    \n",
    "    dfs = [] \n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(X_train_noamr[:, t, :], columns=[f'{feature}_{t}' for feature in features])\n",
    "        dfs.append(temp_df)\n",
    "    final_df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    dls = [] \n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(y_train_noamr[:, t], columns=[t])\n",
    "        dls.append(temp_df)\n",
    "    final_dl = pd.concat(dls, axis=1)\n",
    "    \n",
    "    return final_df, final_dl, T, F\n",
    "\n",
    "def population(split_num, features, norm):\n",
    "    \n",
    "    X_train = np.load(f\"../../DATA/BBCET_cult_ok/s{split_num}/X_train_tensor_{norm}.npy\")\n",
    "    y_train = pd.read_csv(f\"../../DATA/BBCET_cult_ok/s{split_num}/y_train_tensor_{norm}.csv\")\n",
    "    \n",
    "    y_train_aux = y_train[y_train.individualMRGerm != 666].reset_index(drop=True)\n",
    "    y_train_aux = y_train_aux.groupby(by=\"Admissiondboid\").sum().reset_index()\n",
    "    pop = y_train_aux[y_train_aux.individualMRGerm >= 0].index\n",
    "    \n",
    "    X_train_pop = X_train[pop]\n",
    "    P, T, F = X_train.shape\n",
    "    y_train_values = y_train[['individualMRGerm']].values.flatten()\n",
    "    y_train_pop = y_train_values.reshape((P, T))\n",
    "    y_train_pop = y_train_pop[pop]\n",
    "    \n",
    "    dfs = [] \n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(X_train_pop[:, t, :], columns=[f'{feature}_{t}' for feature in features])\n",
    "        dfs.append(temp_df)\n",
    "    final_df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    dls = [] \n",
    "    for t in range(T):\n",
    "        temp_df = pd.DataFrame(y_train_pop[:, t], columns=[t])\n",
    "        dls.append(temp_df)\n",
    "    final_dl = pd.concat(dls, axis=1)\n",
    "    \n",
    "    return final_df, final_dl, T, F\n",
    "\n",
    "############################################################\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "def count_neighbors(tree, x, r):\n",
    "    return tree.query_radius(x, r, count_only=True)\n",
    "\n",
    "def add_noise(x, intens):#-10!!\n",
    "    # small noise to break degeneracy, see doc.\n",
    "    return x + intens * np.random.random_sample(x.shape)\n",
    "\n",
    "def build_tree(points, val):\n",
    "    if points.shape[1] >= val:\n",
    "        return BallTree(points, metric='chebyshev')\n",
    "    return KDTree(points, metric='chebyshev')\n",
    "\n",
    "def query_neighbors(tree, x, k):\n",
    "    return tree.query(x, k=k + 1)[0][:, k]\n",
    "\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    tree = build_tree(points, val)\n",
    "    dvec = dvec - 1e-15\n",
    "    num_points = count_neighbors(tree, points, dvec)\n",
    "    return np.mean(digamma(num_points))\n",
    "## END UTILITY FUNCTIONS ENTROPY\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "### FUNCT D-D (MI y CMI) ###\n",
    "def entropyd(sx, base=2):\n",
    "    \"\"\" Discrete entropy estimator\n",
    "        sx is a list of samples\n",
    "    \"\"\"\n",
    "    unique, count = np.unique(sx, return_counts=True, axis=0)\n",
    "    # Convert to float as otherwise integer division results in all 0 for proba.\n",
    "    proba = count.astype(float) / len(sx)\n",
    "    # Avoid 0 division; remove probabilities == 0.0 (removing them does not change the entropy estimate as 0 * log(1/0) = 0.\n",
    "    proba = proba[proba > 0.0]\n",
    "    return np.sum(proba * np.log(1. / proba)) / log(base)\n",
    "\n",
    "#### MI. D-D\n",
    "\n",
    "def centropyd(x, y, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator for the\n",
    "        entropy of X conditioned on Y.\n",
    "    \"\"\"\n",
    "    xy = np.c_[x, y]\n",
    "    return entropyd(xy, base) - entropyd(y, base)\n",
    "\n",
    "\n",
    "def midd(x, y, base=2):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    return entropyd(x, base) - centropyd(x, y, base)\n",
    "\n",
    "\n",
    "#### Cond. D-D\n",
    "def cmidd(x, y, z, base=2):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y) == len(z), \"Arrays should have same length\"\n",
    "    xz = np.c_[x, z]\n",
    "    yz = np.c_[y, z]\n",
    "    xyz = np.c_[x, y, z]\n",
    "    return entropyd(xz, base) + entropyd(yz, base) - entropyd(xyz, base) - entropyd(z, base)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "## MI - C-D\n",
    "def entropy(x, k, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "        x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = np.asarray(x)\n",
    "    n_elements, n_features = x.shape\n",
    "    x = add_noise(x, intens)\n",
    "    tree = build_tree(x, val)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    const = digamma(n_elements) - digamma(k) + n_features * log(2)\n",
    "    return (const + n_features * np.log(nn).mean()) / log(base)\n",
    "\n",
    "def micd(x, y, k, base=2, warning=True):\n",
    "    \"\"\" If x is continuous and y is discrete, compute mutual information\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    entropy_x = entropy(x, k, base)\n",
    "    y_unique, y_count = np.unique(y, return_counts=True, axis=0)\n",
    "    y_proba = y_count / len(y)\n",
    "\n",
    "    entropy_x_given_y = 0.\n",
    "    for yval, py in zip(y_unique, y_proba):\n",
    "        x_given_y = x[(y == yval).all(axis=1)]\n",
    "        \n",
    "        if k <= len(x_given_y) - 1:\n",
    "            entropy_x_given_y += py * entropy(x_given_y, k, base)\n",
    "        else:\n",
    "            if warning:\n",
    "                warnings.warn(\"Warning, after conditioning, on y={yval} insufficient data. \"\n",
    "                              \"Assuming maximal entropy in this case.\".format(yval=yval))\n",
    "            entropy_x_given_y += py * entropy_x\n",
    "    return abs(entropy_x - entropy_x_given_y)  # units already applied\n",
    "\n",
    "\n",
    "def midc(x, y, k, base=2, warning=True):\n",
    "    return micd(y, x, k, base, warning)\n",
    "\n",
    "## END MI - C-D\n",
    "\n",
    "## CMI - C-D\n",
    "def mi(x, y, z, k, base=2, alpha=0):\n",
    "    \"\"\" \n",
    "    Mutual information of x and y (conditioned on z if z is not None)\n",
    "    x, y should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    x, y = x.reshape(x.shape[0], -1), y.reshape(y.shape[0], -1)\n",
    "    x = add_noise(x, intens)\n",
    "    y = add_noise(y, intens)\n",
    "    points = [x, y]\n",
    "    if z is not None:\n",
    "        z = np.asarray(z)\n",
    "        z = z.reshape(z.shape[0], -1)\n",
    "        points.append(z)\n",
    "    points = np.hstack(points)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = build_tree(points, val)\n",
    "    dvec = query_neighbors(tree, points, k)\n",
    "    if z is None:\n",
    "        a, b, c, d = avgdigamma(x, dvec), avgdigamma(\n",
    "            y, dvec), digamma(k), digamma(len(x))\n",
    "        if alpha > 0:\n",
    "            d += lnc_correction(tree, points, k, alpha)\n",
    "    else:\n",
    "        xz = np.c_[x, z]\n",
    "        yz = np.c_[y, z]\n",
    "        a, b, c, d = avgdigamma(xz, dvec), avgdigamma(\n",
    "            yz, dvec), avgdigamma(z, dvec), digamma(k)\n",
    "    return (-a - b + c + d) / log(base)\n",
    "\n",
    "\n",
    "def cmi(x, y, z, k, base=2):\n",
    "    \"\"\" Mutual information of x and y, conditioned on z\n",
    "        Legacy function. Use mi(x, y, z) directly.\n",
    "    \"\"\"\n",
    "    return mi(x, y, z, k, base=base)\n",
    "## END CMI - C-D\n",
    "\n",
    "######################################## STEP 0 ########################################\n",
    "\n",
    "def firstMI(X, y, k, variable_types, base=2):\n",
    "    maxMI = 0\n",
    "    indexMIMax = 0\n",
    "    \n",
    "    claves = X.columns\n",
    "    for k in range(X.shape[1]):\n",
    "        # Select column in y corresponding to the day of the feature in X (f.e: feature 'AMG_0' with label of day 0)\n",
    "        y_col = y.iloc[:, k // len(variable_types)].values\n",
    "\n",
    "        # Filter the values in X and y to avoid the 666\n",
    "        mask = (X.iloc[:, k].values != 666) & (y_col != 666)\n",
    "        X_filtered = X.iloc[:, k].values[mask].reshape(-1, 1)\n",
    "        y_filtered = y_col[mask].reshape(-1, 1)\n",
    "        \n",
    "        if variable_types[k] == 'discreta':\n",
    "            miValue = np.abs(midd(X_filtered, y_filtered, base=base))\n",
    "        else:\n",
    "            miValue = np.abs(micd(X_filtered, y_filtered, k, base=base))\n",
    "        \n",
    "        if miValue > maxMI:\n",
    "            maxMI = miValue\n",
    "            indexMIMax = k\n",
    "            \n",
    "    # Eliminate the first variable and add it to z\n",
    "    key = X.columns[indexMIMax]\n",
    "    z = X[key].values.reshape(-1, 1)\n",
    "    X = X.drop(columns=[key])\n",
    "    \n",
    "    return X, z, key, maxMI\n",
    "\n",
    "def myCondMI(X, y, z, k, variable_types, base=2):\n",
    "    maxMI = 0\n",
    "    indexMIMax = 0\n",
    "    claves = X.columns\n",
    "    for f in range(X.shape[1]):\n",
    "        # Select column in y corresponding to the day of the feature in X (f.e: feature 'AMG_0' with label of day 0)\n",
    "        y_col = y.iloc[:, f // len(variable_types)].values\n",
    "\n",
    "        # Filter the values in X and y to avoid the 666\n",
    "        mask = (X.iloc[:, f].values != 666) & (y_col != 666)\n",
    "        X_filtered = X.iloc[:, f].values[mask].reshape(-1, 1)\n",
    "        y_filtered = y_col[mask].reshape(-1, 1)\n",
    "        z_filtered = z[mask]\n",
    "                \n",
    "        if variable_types[f] == 'discreta':\n",
    "            miValue = np.abs(cmidd(X_filtered, y_filtered, z_filtered, base=base))\n",
    "        else:\n",
    "            miValue = np.abs(cmi(X_filtered, y_filtered, z_filtered, k, base=base))\n",
    "        \n",
    "        if miValue > maxMI:\n",
    "            maxMI = miValue\n",
    "            indexMIMax = f\n",
    "            \n",
    "    # Eliminate the first variable and add it to z\n",
    "    key = X.columns[indexMIMax]\n",
    "    z = np.append(z, X[key].values.reshape(-1, 1), axis=1)\n",
    "    X = X.drop(columns=[key])\n",
    "    \n",
    "    return X, z, key, maxMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45395803",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_directory = \"./Results_CMI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702573e",
   "metadata": {},
   "source": [
    "# MDR patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be19f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_amr = {}\n",
    "\n",
    "k_n = 1\n",
    "intens = 1e-15\n",
    "val = 35\n",
    "\n",
    "norm = '0robustNorm'\n",
    "\n",
    "for split_num in range(1,4):\n",
    "    features, tipos_variables = reset()\n",
    "    final_df, final_dl, T, F = prepare_amr(split_num, features, norm)\n",
    "    indexesSelected = []\n",
    "    MIvalues = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        features, tipos_variables = reset()\n",
    " \n",
    "        X_day = final_df.iloc[:, t*F:(t+1)*F]\n",
    "        y_day = final_dl.iloc[:, [t]]\n",
    "\n",
    "        for j in range(F):\n",
    "            if j == 0:\n",
    "                X_day, z, featureSelected, maxMI = firstMI(X_day, y_day, k_n, tipos_variables)\n",
    "                maxMI = maxMI/(maxMI*10)\n",
    "            else:\n",
    "                X_day, z, featureSelected, maxMI = myCondMI(X_day, y_day, z, k_n, tipos_variables)\n",
    "            \n",
    "            feat = featureSelected.split(\"_\"+str(t))[0]\n",
    "            idx = features.index(feat)\n",
    "            del features[idx]\n",
    "            del tipos_variables[idx]\n",
    "            \n",
    "            featureSelected = featureSelected.split(\"_\"+str(t))[0]\n",
    "            indexesSelected.append(featureSelected)\n",
    "            MIvalues.append(maxMI)\n",
    "            \n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': indexesSelected,\n",
    "        'MI Value': MIvalues\n",
    "    })\n",
    "    \n",
    "    results_amr[f'results_df_{split_num}'] = results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(split_directory, f\"CMI_results_amr.pkl\"), 'wb') as f:\n",
    "    pickle.dump(results_amr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09482024",
   "metadata": {},
   "source": [
    "# Non MDR patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_noamr = {}\n",
    "intens = 1e-13\n",
    "val = 32\n",
    "\n",
    "for split_num in range(1,4):\n",
    "    features, tipos_variables = reset()\n",
    "    final_df, final_dl, T, F = prepare_noamr(split_num, features, norm)\n",
    "    indexesSelected = []\n",
    "    MIvalues = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        features, tipos_variables = reset()\n",
    "       \n",
    "        X_day = final_df.iloc[:, t*F:(t+1)*F]\n",
    "        y_day = final_dl.iloc[:, [t]]\n",
    "\n",
    "        for j in range(F):\n",
    "            if j == 0:\n",
    "                X_day, z, featureSelected, maxMI = firstMI(X_day, y_day, k_n, tipos_variables)\n",
    "                maxMI = maxMI/(maxMI*10)\n",
    "            else:\n",
    "                X_day, z, featureSelected, maxMI = myCondMI(X_day, y_day, z, k_n, tipos_variables)\n",
    "            \n",
    "            feat = featureSelected.split(\"_\"+str(t))[0]\n",
    "            idx = features.index(feat)\n",
    "            del features[idx]\n",
    "            del tipos_variables[idx]\n",
    "            \n",
    "            featureSelected = featureSelected.split(\"_\"+str(t))[0]\n",
    "            indexesSelected.append(featureSelected)\n",
    "            MIvalues.append(maxMI)\n",
    "            \n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': indexesSelected,\n",
    "        'MI Value': MIvalues\n",
    "    })\n",
    "    \n",
    "    results_noamr[f'results_df_{split_num}'] = results_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(split_directory, f\"CMI_results_noamr.pkl\"), 'wb') as f:\n",
    "    pickle.dump(results_noamr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85773df5",
   "metadata": {},
   "source": [
    "## Population analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pop = {}\n",
    "intens = 1e-13\n",
    "val = 32\n",
    "\n",
    "for split_num in range(1,4):\n",
    "    features, tipos_variables = reset()\n",
    "    final_df, final_dl, T, F = population(split_num, features, norm)\n",
    "    indexesSelected = []\n",
    "    MIvalues = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        features, tipos_variables = reset()\n",
    "        \n",
    "        X_day = final_df.iloc[:, t*F:(t+1)*F]\n",
    "        y_day = final_dl.iloc[:, [t]]\n",
    "\n",
    "        for j in range(F):\n",
    "            if j == 0:\n",
    "                X_day, z, featureSelected, maxMI = firstMI(X_day, y_day, k_n, tipos_variables)\n",
    "                maxMI = maxMI/(maxMI*10)\n",
    "            else:\n",
    "                X_day, z, featureSelected, maxMI = myCondMI(X_day, y_day, z, k_n, tipos_variables)\n",
    "            \n",
    "            feat = featureSelected.split(\"_\"+str(t))[0]\n",
    "            idx = features.index(feat)\n",
    "            del features[idx]\n",
    "            del tipos_variables[idx]\n",
    "            \n",
    "            featureSelected = featureSelected.split(\"_\"+str(t))[0]\n",
    "            indexesSelected.append(featureSelected)\n",
    "            MIvalues.append(maxMI)\n",
    "            \n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': indexesSelected,\n",
    "        'MI Value': MIvalues\n",
    "    })\n",
    "    \n",
    "    results_pop[f'results_df_{split_num}'] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a970c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(split_directory, f\"CMI_results_population.pkl\"), 'wb') as f:\n",
    "    pickle.dump(results_pop, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
