{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6932173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "from rnns_architectures.utils import *\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fa359",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af69ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_label(df):\n",
    "    \"\"\"Groups the DataFrame by 'Admissiondboid' and assigns a single label. \n",
    "    If any patient has a day with 'individualMRGerm' equal to 1 (MDR), \n",
    "    is labeled as 1; otherwise, it is labeled as 0 (Non-MDR).\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped = df.groupby('Admissiondboid')\n",
    "    results = []\n",
    "    \n",
    "    for admissionboid, group in grouped:\n",
    "        if (group['individualMRGerm'] == 1).any():\n",
    "            results.append({'Admissiondboid': admissionboid, 'individualMRGerm': 1})\n",
    "        else:\n",
    "            results.append({'Admissiondboid': admissionboid, 'individualMRGerm': 0})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def weighted_binary_crossentropy(hyperparameters):\n",
    "    w1 = hyperparameters[\"w1\"]\n",
    "    w2 = hyperparameters[\"w2\"]\n",
    "    \"\"\"\n",
    "    Binary form of weighted binary cross entropy.\n",
    "      WBCE(p_t) = -w * (1 - p_t)* log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    Usage:\n",
    "     model.compile(loss=[weighted_binary_crossentropyv2(hyperparameters)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(w1 * (1. - pt_1) * K.log(pt_1)) \\\n",
    "               -K.sum(w2 * (pt_0) * K.log(1. - pt_0))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a GRU model based on several hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - hyperparameters: Dictionary containing the hyperparameters. \n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the compiled model.\n",
    "    \"\"\"\n",
    "    \n",
    "    dynamic_input = tf.keras.layers.Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    masked = tf.keras.layers.Masking(mask_value=hyperparameters['mask_value'])(dynamic_input)\n",
    "\n",
    "    gru_encoder = tf.keras.layers.GRU(\n",
    "        hyperparameters[\"layers\"][1],\n",
    "        dropout=hyperparameters['dropout'],\n",
    "        return_sequences=False,\n",
    "        activation=hyperparameters['activation'],\n",
    "        use_bias=False\n",
    "    )(masked)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, use_bias=False, activation=\"sigmoid\")(gru_encoder)\n",
    "\n",
    "    model = tf.keras.Model(dynamic_input, [output])\n",
    "    model.compile(\n",
    "        loss=weighted_binary_crossentropy(hyperparameters),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"lr_scheduler\"]),\n",
    "        metrics=['accuracy', \"AUC\"],\n",
    "        weighted_metrics = []\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built GRU model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - sample_weights_train, sample_weights_val: numpy.ndarray. Weights for the T and V data to handle class imbalance.\n",
    "        - hyperparameters: Dictionary containing the hyperparameters.\n",
    "        - seed: Integer seed for reproducibility.\n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the trained model.\n",
    "        - hist:  The training history.\n",
    "        - earlystopping: The early stopping callback.\n",
    "    \"\"\"\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    n_epochs_max = hyperparameters['n_epochs_max']\n",
    "\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=hyperparameters[\"mindelta\"],\n",
    "                                                  patience=hyperparameters[\"patience\"],\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  mode=\"min\")\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     callbacks=[earlystopping], batch_size=batch_size, epochs=n_epochs_max,\n",
    "                     verbose=hyperparameters['verbose'])\n",
    "    \n",
    "    return model, hist, earlystopping\n",
    "\n",
    "\n",
    "def calculate_metrics(model, X_test, y_test):\n",
    "\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_test = sklearn.metrics.accuracy_score(y_test.individualMRGerm.values, np.round(y_pred_test))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test.individualMRGerm.values, np.round(y_pred_test)).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(y_test.individualMRGerm.values, y_pred_test)\n",
    "    \n",
    "    # Dataframe\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'accuracy': [accuracy_test],\n",
    "        'specificity': [specificity],\n",
    "        'recall': [recall],\n",
    "        'precision': [precision],\n",
    "        'f1_score': [f1_score],\n",
    "        'roc_auc': [roc_auc]\n",
    "    })\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972c6f2-f374-49aa-91d3-c4538cf2218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def evaluate_combination(k, l, m, b, hyperparameters, dropout, layers, lr_scheduler, adjustment_factor, activation, seed, split, norm, n_time_steps):\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "    hyperparameters_copy['dropout'] = dropout[k]\n",
    "    hyperparameters_copy['layers'] = layers[l]\n",
    "    hyperparameters_copy['lr_scheduler'] = lr_scheduler[m]\n",
    "    hyperparameters_copy['adjustment_factor'] = adjustment_factor[0]\n",
    "    hyperparameters_copy['activation'] = activation[b]\n",
    "    \n",
    "    v_val_loss = []\n",
    "    data_paths = []  # Para guardar las rutas de los archivos\n",
    "\n",
    "    for i in range(5):\n",
    "        # Guardar las rutas a los archivos sin cargar los datos completos en memoria\n",
    "        X_train_path = f\"../../../DATA/MDR/{split}/X_train_tensor_{i}{norm}.npy\"\n",
    "        y_train_path = f\"../../../DATA/MDR/{split}/y_train_tensor_{i}{norm}.csv\"\n",
    "        X_val_path = f\"../../../DATA/MDR/{split}/X_val_tensor_{i}{norm}.npy\"\n",
    "        y_val_path = f\"../../../DATA/MDR/{split}/y_val_tensor_{i}{norm}.csv\"\n",
    "\n",
    "        data_paths.append((X_train_path, y_train_path, X_val_path, y_val_path))\n",
    "\n",
    "        X_train = np.load(X_train_path)\n",
    "        y_train = pd.read_csv(y_train_path)\n",
    "        y_train = single_label(y_train)\n",
    "        X_val = np.load(X_val_path)\n",
    "        y_val = pd.read_csv(y_val_path)\n",
    "        y_val = single_label(y_val)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        reset_keras()\n",
    "\n",
    "        model, hist, early = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train.individualMRGerm.values, \n",
    "            y_val.individualMRGerm.values,\n",
    "            hyperparameters_copy,\n",
    "            seed\n",
    "        )\n",
    "\n",
    "        v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "        del X_train, y_train, X_val, y_val, model, hist, early\n",
    "        K.clear_session() \n",
    "        gc.collect()  \n",
    "\n",
    "    metric_dev = np.mean(v_val_loss)\n",
    "    gc.collect()\n",
    "    \n",
    "    return (metric_dev, k, l, m, b, data_paths)\n",
    "\n",
    "\n",
    "\n",
    "def myCVGridParallel(hyperparameters, dropout, lr_scheduler, layers, adjustment_factor, activation, seed, split, norm, n_time_steps=14):\n",
    "    bestHyperparameters = {}\n",
    "    bestMetricDev = np.inf\n",
    "    best_data_paths = [] \n",
    "\n",
    "    results = Parallel(n_jobs=12)(\n",
    "        delayed(evaluate_combination)(k, l, m, b, hyperparameters, dropout, layers, lr_scheduler, adjustment_factor, activation, seed, split, norm, n_time_steps)\n",
    "        for k in range(len(dropout))\n",
    "        for l in range(len(layers))\n",
    "        for m in range(len(lr_scheduler))\n",
    "        for b in range(len(activation))\n",
    "    )\n",
    "\n",
    "    for metric_dev, k, l, m, b, data_paths in results:\n",
    "        if metric_dev < bestMetricDev:\n",
    "            bestMetricDev = metric_dev\n",
    "            bestHyperparameters = {\n",
    "                'dropout': dropout[k],\n",
    "                'layers': layers[l],\n",
    "                'lr_scheduler': lr_scheduler[m],\n",
    "                'adjustment_factor': adjustment_factor[0],\n",
    "                'activation': activation[b]\n",
    "            }\n",
    "            best_data_paths = data_paths\n",
    "            \n",
    "    return bestHyperparameters, best_data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096538e8",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134ccbc",
   "metadata": {},
   "source": [
    "- **seeds**: Seed values to ensure reproducibility.\n",
    "- **input_shape**: Number of features in each time step of the input data.\n",
    "- **n_time_steps**: Number of time steps in the input sequence.\n",
    "- **batch_size**: Number of batches for training.\n",
    "- **n_epochs_max**: Maximum number of epochs for training.\n",
    "- **layer_list**: A list with different configurations for the layers of the model.\n",
    "- **dropout**: Dropout rates.\n",
    "- **lr_scheduler**: Learning rates.\n",
    "- **norm**: Type of normalization applied to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9, 18, 35]\n",
    "\n",
    "input_shape = 71\n",
    "n_time_steps = 14\n",
    "batch_size = 32\n",
    "n_epochs_max = 1000\n",
    "\n",
    "layer_list = [\n",
    "    [input_shape, 3, 1], [input_shape, 5, 1], [input_shape, 10, 1],\n",
    "    [input_shape, 20, 1],  [input_shape, 30, 1], [input_shape, 40, 1], \n",
    "    [input_shape, 50, 1], [input_shape, 60, 1]\n",
    "]\n",
    "\n",
    "dropout = [0.0, 0.15, 0.3]\n",
    "lr_scheduler = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "\n",
    "adjustment_factor = [1]  \n",
    "\n",
    "\n",
    "activation = ['tanh', 'LeakyReLU']\n",
    " \n",
    "norm = \"robustNorm\"\n",
    "\n",
    "w2 = 0.18\n",
    "w1 = 0.82\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"w1\":w1, \"w2\":w2, \n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": 50,\n",
    "    \"dropout\": 0.0,\n",
    "    \"verbose\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59cdc4",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa89786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "  \n",
    "\n",
    "run_model = True \n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    for i in [1, 2, 3]:\n",
    "        init = time.time()\n",
    "        # LOAD TEST AND PRE-TRAIN\n",
    "        X_test = np.load(\"../../../DATA/MDR/s\" + str(i) + \"/X_test_tensor_\" + norm + \".npy\")\n",
    "        y_test = pd.read_csv(\"../../../DATA/MDR/s\" + str(i) + \"/y_test_tensor_\" + norm + \".csv\")\n",
    "        y_test = single_label(y_test)\n",
    "        #GridSearch of hyperparameters \n",
    "        bestHyperparameters, best_data_paths = myCVGridParallel(hyperparameters,\n",
    "                                                               dropout,\n",
    "                                                               lr_scheduler,\n",
    "                                                               layer_list,\n",
    "                                                               adjustment_factor,\n",
    "                                                               activation,\n",
    "                                                               seeds[i-1],\n",
    "                                                               \"s\"+str(i),\n",
    "                                                               norm)\n",
    "        fin = time.time()\n",
    "\n",
    "        X_train_path, y_train_path, X_val_path, y_val_path = best_data_paths[0] \n",
    "    \n",
    "        # Cargar los datos desde las rutas\n",
    "        X_train = np.load(X_train_path)\n",
    "        y_train = pd.read_csv(y_train_path)\n",
    "        y_train = single_label(y_train)\n",
    "        X_val = np.load(X_val_path)\n",
    "        y_val = pd.read_csv(y_val_path)\n",
    "        y_val = single_label(y_val)\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_GRU/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters = {\n",
    "            'n_time_steps': hyperparameters[\"n_time_steps\"],\n",
    "            'mask_value': hyperparameters[\"mask_value\"],\n",
    "            \"w1\":hyperparameters[\"w1\"], \"w2\":hyperparameters[\"w2\"],\n",
    "            'batch_size': hyperparameters[\"batch_size\"],\n",
    "            'n_epochs_max': hyperparameters[\"n_epochs_max\"],\n",
    "            'monitor':  hyperparameters[\"monitor\"],\n",
    "            \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "            \"patience\": hyperparameters[\"patience\"],\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"adjustment_factor\": bestHyperparameters[\"adjustment_factor\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"],\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "        # --- TRY ON TEST -----------------------------------------------------------------------\n",
    "\n",
    "        reset_keras()\n",
    "\n",
    "        model, hist, early = run_network(\n",
    "            X_train, \n",
    "            X_val,\n",
    "            y_train.individualMRGerm.values, \n",
    "            y_val.individualMRGerm.values,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_loss'])\n",
    "        \n",
    "        #Calculate metrics and predictions\n",
    "        y_pred = model.predict(x=X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        \n",
    "        metrics = calculate_metrics(model, X_test, y_test)\n",
    "\n",
    "        # Save y_pred and metrics for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "        with open(os.path.join(split_directory, f\"model_split_{i}.h5\"), 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(os.path.join(split_directory, f\"metrics_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(metrics, f)\n",
    "\n",
    "\n",
    "    # # END EXECUTION - SAVE AGGREGATED RESULTS\n",
    "    print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ec8349-80c5-4229-a111-a38b9a7e2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './Results_GRU'\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "y_pred_by_split = {}\n",
    "\n",
    "y_pred_by_split_1 = load_from_pickle(os.path.join('./Results_GRU/split_1', \"metrics_1.pkl\"))\n",
    "y_pred_by_split_2 = load_from_pickle(os.path.join('./Results_GRU/split_2', \"metrics_2.pkl\"))\n",
    "y_pred_by_split_3 = load_from_pickle(os.path.join('./Results_GRU/split_3', \"metrics_3.pkl\"))\n",
    "\n",
    "y_pred_by_split['1'] = y_pred_by_split_1\n",
    "y_pred_by_split['2'] = y_pred_by_split_2\n",
    "y_pred_by_split['3'] = y_pred_by_split_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff90881-0a94-4bea-aedf-c5bacf8532f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.822346</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.381323</td>\n",
       "      <td>0.475728</td>\n",
       "      <td>0.793231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  specificity    recall  precision  f1_score   roc_auc\n",
       "0  0.794286     0.822346  0.632258   0.381323  0.475728  0.793231"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_by_split_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bed966-12d2-4ae8-9858-2ae237cf13e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.779048</td>\n",
       "      <td>0.804299</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.479821</td>\n",
       "      <td>0.800789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  specificity    recall  precision  f1_score   roc_auc\n",
       "0  0.779048     0.804299  0.644578   0.382143  0.479821  0.800789"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_by_split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ee073a-73e7-415a-98db-f6c4782f0583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.551429</td>\n",
       "      <td>0.571101</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>0.178022</td>\n",
       "      <td>0.255924</td>\n",
       "      <td>0.557214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  specificity    recall  precision  f1_score   roc_auc\n",
       "0  0.551429     0.571101  0.455056   0.178022  0.255924  0.557214"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_by_split_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593db22-4b22-4572-b4c2-ba63ced003fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
